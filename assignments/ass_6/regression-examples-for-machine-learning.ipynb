{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This document includes below examples;\n1. [**Linear Regression (LR)**](#1)\n   1. [Prediction](#11)\n   1. [R Square (LR)](#12)\n1. [**Multiple Linear Regression**](#2)\n   1. [Prediction](#21)\n   1. [R Square (LR)](#22)\n1. [**Polynomial Regression (PR)**](#3)\n   1. [Prediction](#31)\n   1. [R Square (LR)](#32)\n1. [**Decision Tree Regression (DTR)**](#4)\n   1. [Prediction](#41)\n   1. [R Square (LR)](#42)\n1. [**Random Forest Regression (RFR)**](#5)\n   1. [Prediction](#51)\n   1. [R Square (LR)](#52)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the data\ndframe = pd.read_csv(\"../input/Admission_Predict_Ver1.1.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dframe.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check whether there are empty rows or not.\ndframe.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dframe.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation \n# We excluded \"Serial No\" with data.iloc[:,1:]) )\n\ndframe.iloc[:,1:].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation map\nf, axx = plt.subplots(figsize=(10,10))\nsns.heatmap(dframe.iloc[:,1:].corr(), linewidths=0.5, cmap=\"Blues\", annot=True,fmt=\".1f\", ax=axx)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CGPA, GRE Score and TOEFL Scores are 3 most correlated features for the \"Chance of Admit\".\n\nLet's drop all the duplicated values from the data frame.","metadata":{}},{"cell_type":"code","source":"# Drop the duplicated values of the Chance of Admit.\ndf= dframe.drop_duplicates(subset=[\"Chance of Admit \"])\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= df.drop_duplicates(subset=\"CGPA\")\ndf= df.drop_duplicates(subset=\"GRE Score\")\ndf= df.drop_duplicates(subset=\"TOEFL Score\")\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation \n# We excluded \"Serial No\" with data.iloc[:,1:]) )\n\ndf.iloc[:,1:].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation map\nf, axx = plt.subplots(figsize=(10,10))\nsns.heatmap(df.iloc[:,1:].corr(), linewidths=0.5, cmap=\"Blues\", annot=True,fmt=\".2f\", ax=axx)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the new data frame (with non-duplicated values);\n\nTOEFL Scores, CGPA and GRE Score are 3 most correlated features for the \"Chance of Admit\".","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean value of \"Chance of Admit \" is 0.677368.\n# Output is on above; df.describe()\n\n# Create a new column for High and Low.\n\ndf[\"Admit Level\"] = [\"Low\" if each < 0.677368 else \"High\" for each in df[\"Chance of Admit \"]]\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vizualization\n# CGPA, GRE Score and TOEFL Scores / Chance of Admit\n\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter(\n                        x = df[\"Chance of Admit \"],\n                        y = df.CGPA,\n                        mode = \"markers\",\n                        name = \"CGPA\",\n                        marker = dict(color=\"rgba(255, 100, 128, 0.8)\"),\n                        text = df[\"Admit Level\"]\n                        )\ntrace2 = go.Scatter(\n                        x = df[\"Chance of Admit \"],\n                        y = df[\"GRE Score\"],\n                        mode = \"markers\",\n                        name = \"GRE Score\",\n                        marker = dict(color=\"rgba(80, 80, 80, 0.8)\"),\n                        text = df[\"Admit Level\"]\n                        )\ntrace3 = go.Scatter(\n                        x = df[\"Chance of Admit \"],\n                        y = df[\"TOEFL Score\"],\n                        mode = \"markers\",\n                        name = \"TOEFL Score\",\n                        marker = dict(color=\"rgba(0, 128, 255, 0.8)\"),\n                        text = df[\"Admit Level\"]\n                        )\ndata = [trace1, trace2, trace3]\nlayout = dict(title=\"CGPA, GRE Score and TOEFL Scores v Chance of Admit\",\n             xaxis=dict(title=\"Chance of Admit\", ticklen=5, zeroline=False),\n             yaxis=dict(title=\"Values\", ticklen=5, zeroline=False)\n             )\nfig = dict(data=data, layout=layout)\niplot(fig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n1. **Linear Regression (LR)**\n\ny = b0 + b1*x","metadata":{}},{"cell_type":"code","source":"# Sklearn library\nfrom sklearn.linear_model import LinearRegression\n\nlinear_reg = LinearRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the fit operation, we need to use numpy arrays on the x and y axixes, so we will use \"df.ColumnName.values\". But, its type will be \"(500,)\" so we will reshape it. The reason we write -1 is because we may don't know the size, we only need to set the second value to 1.\n\nThe most correlated feature with \"Chance of Admit\" is \"CGPA\".","metadata":{}},{"cell_type":"code","source":"print(df.CGPA.values.shape)\nprint(df[\"Chance of Admit \"].values.shape)\n\n# Reshape\nx = df.CGPA.values.reshape(-1,1)\ny = df[\"Chance of Admit \"].values.reshape(-1,1)\nprint(\"After resphape:\\nX:\", x.shape)\nprint(\"Y:\", y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can use above x&y axises on the fit operation of the linear regression model.","metadata":{}},{"cell_type":"code","source":"linear_reg.fit(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Formula\n# y = b0 + b1*x\n\nb0 = linear_reg.intercept_\nprint(\"b0:\", b0) # the spot where the linear line cuts the y-axis\n\nb1 = linear_reg.coef_\nprint(\"b1:\", b1) # slope\n\nprint(\"Linear Regression Formula:\", \"y = {0} + {1}*x\".format(b0,b1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n**1.1. Prediction**\n\nWe will predict the values according to linear_reg model.","metadata":{}},{"cell_type":"code","source":"x[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CGPA-9.65 = Chance of Admit -0.92\ndf[df.CGPA == 9.65].loc[:,\"Chance of Admit \"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_reg.predict([[9.8]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(min(x), max(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CGPA values that will be predicted.\n\n# Chance of Admit (predicted values)\ny_head = linear_reg.predict(x)\n\nplt.figure(figsize=(10,10))\nplt.scatter(x,y, alpha=0.7)  # Real values (blue)\nplt.plot(x,y_head, color=\"red\") # Predicted values for numpay array (arr).\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n**1.2. R Square (LR)**\n\nWe can evaluate the linear regression model performance with R Square.\n* y: Chance of Admit values\n* y_head: predicted Chance of Admit value\n\nFirst, we must be sure that y and y_head values are using the same number of samples. If not, we will get an error like this:\n\nValueError: Found input variables with inconsistent numbers of samples: [500, 312]","metadata":{}},{"cell_type":"code","source":"# Same shapes\nprint(y.shape, y_head.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R Square Library\nfrom sklearn.metrics import r2_score\n# y: Chance of Admit values\n# y_head: predicted Chance of Admit values with LR\nprint(\"r_square score: \", r2_score(y, y_head))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Success ratio is around **% 70** for the LR prediction.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n2. **Multiple Linear Regression**\n\ny = b0 + b1x1 + b2x2 + ... bnxn","metadata":{}},{"cell_type":"code","source":"# Sklearn library\n# we already imported -- > from sklearn.linear_model import LinearRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define and reshape the variables\n\nx1 = df.loc[:, [\"CGPA\", \"GRE Score\", \"TOEFL Score\"]]\ny1 = df[\"Chance of Admit \"].values.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creat the model and fit the x&y values.\nmultiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x1,y1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Formula\n# y = b0 + b1*x1 + b2*x2 + ... bn*xn\nb0 = multiple_linear_regression.intercept_\nb1,b2,b3 = zip(*multiple_linear_regression.coef_) \nprint(\"b1:\", b1, \"b2:\", b2, \"b3:\", b3)\nprint(\"b0:\", multiple_linear_regression.intercept_)\nprint(\"b1, b2:\", multiple_linear_regression.coef_)\nprint(\"Multiple Linear Regression Formula:\", \"y = {0} + {1}*x1 + {2}*x2 + {3}*x3\".format(b0,b1,b2,b3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"21\"></a> <br>\n**2.1. Prediction**","metadata":{}},{"cell_type":"code","source":"print(\"CGPA:\", min(x1[\"CGPA\"]),\"-\", max(x1[\"CGPA\"]))\nprint(\"GRE Score:\", min(x1[\"GRE Score\"]),\"-\", max(x1[\"GRE Score\"]))\nprint(\"TOEFL Score:\", min(x1[\"TOEFL Score\"]), \"-\", max(x1[\"TOEFL Score\"]))\nplt.figure(figsize=(10,5))\nplt.scatter(df[\"Chance of Admit \"], df.CGPA, color=\"blue\", label=\"CGPA\")\nplt.scatter(df[\"Chance of Admit \"], df[\"GRE Score\"], color=\"green\", label=\"GRE Score\")\nplt.scatter(df[\"Chance of Admit \"], df[\"TOEFL Score\"], color=\"orange\", label=\"TOEFL Score\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1st: CGPA: 6.8 - 9.92\n# 2nd: GRE Score: 290 - 340\n# 3rd: TOEFL Score: 92 - 120\n# Prediction: Chance of Admit\n\nprint(\"Values= np.array( [[6,280,90]])) Prediction =\",\n      multiple_linear_regression.predict(np.array( [[6,280,90]])))\n\nprint(\"Values= np.array( [[8,300,100]])) Prediction =\",\n      multiple_linear_regression.predict(np.array( [[8,300,100]])))\n\nprint(\"Values= np.array( [[10,350,130]])) Prediction =\",\n      multiple_linear_regression.predict(np.array( [[10,350,130]])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"y1_head keeps the prediction values of x1 which has CGPA, GRE Score and\tTOEFL Score values.  ","metadata":{}},{"cell_type":"code","source":"y1_head = multiple_linear_regression.predict(x1)\ny1_head[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,20))\n\nplt.scatter(y, x1.iloc[:,0], color=\"blue\", alpha=0.7) # CGPA\nplt.scatter(y1_head, x1.iloc[:,0], color=\"black\", alpha=0.7)\n\nplt.scatter(y, x1.iloc[:,1], color=\"green\", alpha=0.7) # GRE Score\nplt.scatter(y1_head, x1.iloc[:,1], color=\"black\", alpha=0.7)\n\nplt.scatter(y, x1.iloc[:,2],color=\"orange\", alpha=0.7) # TOEFL  Score\nplt.scatter(y1_head, x1.iloc[:,2], color=\"black\", alpha=0.7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Black values shows the predicted values, other colors are the real values. As you can see, the predicted values are converging to the real values.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"22\"></a> <br>\n**2.2. R Square**","metadata":{}},{"cell_type":"code","source":"# R Square Library\n\n# Imported on previous sections\n# from sklearn.metrics import r2_score\n\n# y: Chance of Admit values\n# y1_head: predicted Chance of Admit values with MLR\nprint(\"r_square score: \", r2_score(y,y1_head))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Success ratio is **% 75** for the  MLR prediction.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n3. **Polynomial Regression (PR)**","metadata":{}},{"cell_type":"markdown","source":"y = b0 + b1x+ b2x^2 + b3x^3 + ... + bnx^n\n","metadata":{}},{"cell_type":"code","source":"# Sklearn library \nfrom sklearn.preprocessing import PolynomialFeatures\n\n# We have chose the second degree equation with (degree=2)\npolynomial_regression = PolynomialFeatures(degree=2)\n# y = b0 + b1*x + b2*x^2\nx = df[\"TOEFL Score\"].values.reshape(-1,1)\n# y = df[\"Chance of Admit \"].values.reshape(-1,1)\nx_ploynominal = polynomial_regression.fit_transform(x)\n\nlinear_regression_poly = LinearRegression()\nlinear_regression_poly.fit(x_ploynominal, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"31\"></a> <br>\n**3.1. Prediction**","metadata":{}},{"cell_type":"code","source":"# Linear Regression (LR) section: x = df.CGPA.values.reshape(-1,1)\n# Linear Regression (LR) section: y = df[\"Chance of Admit \"].values.reshape(-1,1)\nprint(\"x:\\n\", x[:5], \"\\ny:\\n\",y[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicted values\ny_head_poly = linear_regression_poly.predict(x_ploynominal)\ny_head_poly[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(x, y, color=\"blue\", alpha=0.7) # CGPA\nplt.scatter(x, y_head_poly, label=\"poly (degree=2)\", color=\"black\") # predicted Chance of Admit\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"chance\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2nd degre equationd didn't give a proper model. But, we can modify the degree of the equation to converge to real values.","metadata":{}},{"cell_type":"code","source":"# y = b0 + b1*x + b2*x^2 + ..... b10*x^10\npolynomial_regression7 = PolynomialFeatures(degree=7)\n\n# x = df.CGPA.values.reshape(-1,1)\nx_ploynominal_7 = polynomial_regression7.fit_transform(x)\n\nlinear_regression_poly_7 = LinearRegression()\nlinear_regression_poly_7.fit(x_ploynominal_7, y)\n\n# Predicted values\ny_head_poly_7 = linear_regression_poly_7.predict(x_ploynominal_7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y = b0 + b1*x + b2*x^2 + ..... b30*x^30\npolynomial_regression30 = PolynomialFeatures(degree=30)\n\n# x = df.CGPA.values.reshape(-1,1)\nx_ploynominal_30 = polynomial_regression30.fit_transform(x)\n\nlinear_regression_poly_30 = LinearRegression()\nlinear_regression_poly_30.fit(x_ploynominal_30, y)\n\n# Predicted values\ny_head_poly_30 = linear_regression_poly_30.predict(x_ploynominal_30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare the predicted values of different equations.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.scatter(x, y, color=\"blue\", alpha=0.7) # TOEFL Score\nplt.scatter(x, y_head_poly, label=\"poly (degree=2)\", color=\"black\", alpha=\"0.7\") # predicted Chance of Admit\nplt.scatter(x, y_head_poly_7, label=\"poly (degree=7)\", color=\"red\", alpha=\"0.7\") # predicted Chance of Admit\nplt.scatter(x, y_head_poly_30, label=\"poly (degree=30)\", color=\"green\", alpha=\"0.7\") # predicted Chance of Admit\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"chance\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that red predicted values (degree=7) are more convergent on the bottom-left of the graph and they are similar with the green predicted values (degre=30) in the middle and upper-right of the graph.\n\nThe most proper degree may differ between different datas. We don't always have to increase it to get more accurate prediction.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"32\"></a> <br>\n**3.2. R Square**","metadata":{}},{"cell_type":"code","source":"# R Square Library\n\n# Imported on previous sections\n# from sklearn.metrics import r2_score\n\nprint(\"r_square score for degree=2: \", r2_score(y, y_head_poly))\nprint(\"r_square score for degree=7: \", r2_score(y, y_head_poly_7))\nprint(\"r_square score for degree=30: \", r2_score(y, y_head_poly_30))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Success ratio is **%76** for degree=2 and it is **%78.84** for degree=7 for PR.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n4. **Decision Tree Regression (DTR)**","metadata":{}},{"cell_type":"markdown","source":"\"Decision Tree Regression\" method divides the areas between values based on the conditions, assing the average value of the values for each area which is called \"leaf\".\n\nFor example let's take below tree as an example. We can divide the conditions as [0,30), (30,40), (40,50) and (50,100].\n\nIf x = 51 or 100, it means y is 100. It shows that y values for all \"x>50\" means 100 for the model. \nIf x = 31 or 39, it means y is 35. (Y values are chosen arbitrary, it doesnt have to be linear propotional.)\n\n\nx=[0, 100]\n\n                          x1 > 50\n                yes                no\n               y=100              x>30\n                              yes       no\n                              x<40      y=25\n                            yes   no\n                            y=35  y=45","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree Library\nfrom sklearn.tree import DecisionTreeRegressor\n\nx = df[\"TOEFL Score\"].values.reshape(-1,1)\ny = df[\"Chance of Admit \"].values.reshape(-1,1)\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(df[\"TOEFL Score\"] , df[\"Chance of Admit \"],alpha=0.8)\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"41\"></a> <br>\n**4.1. Prediction**","metadata":{}},{"cell_type":"code","source":"y_head_dtr = tree_reg.predict(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(x, y, color=\"blue\", alpha = 0.7)\nplt.scatter(x, y_head_dtr, color=\"black\", alpha = 0.4)\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Black dots are predicted values and they overlap the real values. Because our prediction values (x) are same with the real values. So, we need some other range to predict according to real x values.","metadata":{}},{"cell_type":"code","source":"# Let's make a new array in the range of TOEFL Score values increased by 0.01\nx001= np.arange(min(x), max(x), 0.01).reshape(-1,1) # (start, end, increase value)\ny_head001dtr = tree_reg.predict(x001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(np.unique(y_head001dtr))\n\n# 19 unique values for all values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.scatter(x,y, color=\"blue\", s=100, label=\"real TOEFL Score\") # real y (Chance of Admit) values\nplt.scatter(x001,y_head001dtr, color=\"red\", alpha = 0.7, label=\"predicted TOEFL Score\") # to see the predicted values one by one\nplt.plot(x001,y_head001dtr, color=\"black\", alpha = 0.7)  # to see the average values for each leaf.\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Black plot shows the predicted values, red points show them one by one. As you can see, TOEFL Score values (x) divided between leaves (ranges) and each leaf have an average value as the predicted value. So, we see a constant line for each leaf.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"42\"></a> <br>\n**4.2. R Square**","metadata":{}},{"cell_type":"code","source":"# Same shapes, y and y_head_dtr\nprint(y.shape, y_head_dtr.shape, y_head001dtr.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nprint(\"r_score: \", r2_score(y,y_head_dtr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Success ratio is **%100** for DTR.","metadata":{}},{"cell_type":"markdown","source":"**sklearn.tree.DecisionTreeRegressor:**\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n\n**score(self, X, y, sample_weight=None)**\n\nReturns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n\nParameters:\t\nX : array-like, shape = (n_samples, n_features)\nTest samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator.\n\ny : array-like, shape = (n_samples) or (n_samples, n_outputs)\nTrue values for X.\n\nsample_weight : array-like, shape = [n_samples], optional\nSample weights.\n\nReturns:\t\nscore : float\nR^2 of self.predict(X) wrt. y.\n\nNotes\n\nThe R2 score used when calling score on a regressor will use multioutput='uniform_average' from version 0.23 to keep consistent with metrics.r2_score. This will influence the score method of all the multioutput regressors (except for multioutput.MultiOutputRegressor). To specify the default value manually and avoid the warning, please either call metrics.r2_score directly or make a custom scorer with metrics.make_scorer (the built-in scorer 'r2' uses multioutput='uniform_average').","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n#cross_val_score(tree_reg, boston.data, boston.target, cv=10)\nprint(tree_reg.score(x001, y_head001dtr))\nprint(tree_reg.score(x, y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"r_score: \", r2_score(y,y_head_dtr))\n\nfrom sklearn.model_selection import cross_val_score\nprint(tree_reg.score(x001, y_head001dtr))\nprint(tree_reg.score(x, y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n5. **Random Forest Regression (RFR)**","metadata":{}},{"cell_type":"markdown","source":"Random forest regression combined by  multiple regression. \n\nIt chooses n examples, divides the data to sub datas and uses multiple trees.\n\n                     data\n                       |\n                       |\n                    n sample\n                       |\n                       |\n                    sub_data\n         tree1   tree2  tree3 .... tree n\n         ________________________________\n        |           average               |\n         ________________________________\n                     result\n                     \n            \n                 \n                 \n\nRandomForestRegressor(**n_estimators** = 100, **random_state** = 42)\n\nThis means we will use 100 tree (DTR) and 42 sample. The algorithm chooses the n samples randomly. We gave a constant number for the random state, therefore the algorithm will select the same 42 examples on the next time.","metadata":{"trusted":true}},{"cell_type":"code","source":"plt.scatter(x, y, color=\"blue\", alpha = 0.7)\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"51\"></a> <br>\n**5.1. Prediction**","metadata":{}},{"cell_type":"code","source":"x = df[\"TOEFL Score\"].values.reshape(-1,1)\ny = df[\"Chance of Admit \"].values.reshape(-1,1)\n\nprint(min(x), max(x))\nprint(min(y), max(y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Regression Library\n\nfrom sklearn.ensemble import RandomForestRegressor\n \nrandom_forest_reg = RandomForestRegressor(n_estimators = 100, random_state = 42)\n# n_estimators = 100 --> Tree number\n# random_state = 42  --> Sample number\nrandom_forest_reg.fit(x,y)\n\nprint(random_forest_reg.predict([[98]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# New prediction examples with (Start, End, Increase)\nx001 = np.arange(min(x), max(x), 0.01).reshape(-1,1)\ny_head001rf = random_forest_reg.predict(x001)\n\nprint(min(x001), max(x001))\nprint(min(y_head001rf), max(y_head001rf))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(np.unique(y_head001rf))\n\n\n# 46 unique values for all values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.scatter(x,y, color=\"blue\", label=\"real TOEFL Score\")\nplt.scatter(x001,y_head001rf, color=\"red\", label=\"predicted TOEFL Score\")\nplt.plot(x001,y_head001rf, color=\"black\")\nplt.legend()\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest regression (RFR) uses more trees (100), it gave more accurate predicted values than Decision Tree Regression (DTR).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"52\"></a> <br>\n**5.2. R Square**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nprint(tree_reg.score(x001, y_head001rf))\nprint(tree_reg.score(x, y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ny_headrf = random_forest_reg.predict(x)\nprint(\"r_score: \", r2_score(y,y_headrf))\n\nfrom sklearn.model_selection import cross_val_score\nprint(tree_reg.score(x001, y_head001rf))\nprint(tree_reg.score(x, y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}